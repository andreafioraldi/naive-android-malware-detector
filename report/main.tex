% !TeX encoding = UTF-8
% !TeX program = pdflatex

\documentclass[12pt]{article}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{amsmath}

\usepackage{pgfplots}
\pgfplotsset{compat=1.14}

\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\renewcommand{\baselinestretch}{1.15} 

\title{{\bf Android Malware Detection Report} \\ \bigskip \large HW1 - Machine Learning class \\ \large Sapienza University of Rome}
\author{Andrea Fioraldi 1692419}
%\pagenumbering{roman}

\begin{document}
\maketitle

\begin{abstract}

The detection of malware plays a key role in Cybersecurity nowadays.
The amount of malicious program is increasing exponentially over time.
Malware programs become cheaper and the attack surface becomes bigger.
In the Q3 2016 report \cite{q3} alone, 18 million new malware samples were captured. Also, as stated in the Symantec 2018 ISTR \cite{symantec}, more than 24K of malicious mobile applications are blocked every day and, according to FBI \cite{fbi}, more than 4000 ransomware attacks occur every day.

As the number of attacks increased over time, it looks like the number of people working against cyber crimes is not increasing proportionally. From the moment an attack is discovered, it is a race against time to understand how the malware works, what was compromised by the malware and find a fix. When the number of malware is extremely high the effort spent by the people working on analyzing them is not enough.

In this scenario, an automated approach to analyze the behavior of a malware is reasonable. 
In fact, to address the problem, {\em Machine Learning} is used with notable results.

In this report, we will discuss an approach based on supervised learning in order to classify an Android application as a malware or not.

\end{abstract}

\newpage

\section{Introduction}

To be able to detect if an Android application is a malware using Machine Learning we considered to analyze a very large dataset. The dataset used is {\em DREBIN}, from \cite{conf/ndss/ArpSHGR14}, a labeled dataset of circa 123K safe Android applications and 5560 malicious.
The description of each program is composed of features belonging to 8 different classes.

\begin{enumerate}
\item Requested hardware components (GPS, camera, ...);
\item Requested Permissions (send SMS, access to contacts,...);
\item App components (activities, services, content providers, broadcast receivers);
\item Filtered Intents (Inter Process Communications handled by the sample e.g. \verb|BOOT_COMPLETED|);
\item Restricted API calls (API calls whose access require a permission);
\item Used permissions (permissions effectively used by the application);
\item Suspicious API calls (API calls who allow access to sensitive data e.g. \verb|getDeviceId()|);
\item Network addresses (URLs embedded in the code);
\end{enumerate}

The features from the first four classes are extracted from the \verb|Manifest.xml|, the features from the last classes are extracted from the disassembled Dalvik bytecode.

Inside the dataset, the features are represented using this pattern: \verb|tag::value|.
The tags are 11 and they are associated with the features classes in the following way:

\bigskip
\begin{tabular}{ | l | l | }
        \hline
        \textbf{Class} & \textbf{Tags} \\ \hline
    Hardware components & \verb|feature| \\ \hline
    Requested permission & \verb|permission| \\ \hline
    App components & \verb|activity| \\
     & \verb|service_receiver| \\
     & \verb|provider| \\
     & \verb|service| \\ \hline
    Filtered Intents & \verb|intent| \\ \hline
    Restricted API calls & \verb|api_call| \\ \hline
    Used permissions & \verb|real_permission| \\ \hline
    Suspicious API calls & \verb|call| \\ \hline
    Network addresses & \verb|url| \\ \hline
\end{tabular}
\bigskip

\section{Preprocessing}

We divided the dataset into two parts, one for the training and one for the evaluation. The training set is 2/3 of the original dataset and the evaluation set is the remaining 1/3. We decided also to keep the good/malicious rate the same in the two set.

This division is performed choosing random elements, so running different times the preprocessing phase will affect the final results a bit.

The total number of different features is 6836. Performing some preprocessing we are able to consistently reduce the number of features.

Firstly, we devise to exclude from the training set the features that occur only one time. This choice allows us to reduce the number of features to a more feasible number, 2897.
Secondly, regards the URLs class of features, we simplified the meaning of this class considering as a feature only the hostname of a given URL (e.g. http://malicious.site/endpoint becomes only malicious.site).

\section{Model}

The chosen model is the {\em Naive Bayes classifier}, a probabilistic model used mainly for text classification. In this model, as explained in \cite{bayes}, a document is viewed as a collection of words and the order of the words in a document is not relevant.

The list of features of an application can be processed like a list of words in a document without losing information, so this model is suitable for our problem.

In our case we divided the applications in two classes $c_1$ and $c_2$, respcetively {\em malware} and {\em notmalware}.


We considered two variants of Naive-Bayes, the {\em bernoulli} and the {\em multinomial}.

%A probability of a class $c$, in our case {\em malware} or {\em notmalware}, given a document $d$ is $

The Naive-Bayes model classifies an instance $x$, represented as a set of features $<x_0 ... x_n>$, with the class $c_m$ which maximizes the probability $P (c_m | x, D)$, where $D$ is the dataset.
The class is computed using the following function:

\bigskip
$argmax_{c \in C}\{P (c | x, D)\}$
\bigskip

that is equal to:

\bigskip
$argmax_{c \in C}\{P (x | c, D)P (c | D)\}$
\bigskip

And, assuming the conditional independece of the features (words) $x_i$, is also equal to:

\bigskip
$argmax_{c \in C}\{P (c | D) \Pi_i P (x_i | c,D)\}$
\bigskip

In the multinomial variant, this function becomes:

\bigskip
$argmax_{c \in C}\{ \displaystyle \frac{P(c | D) \Pi_i P(x_i | c, D)^{n_{x_i x}}}{P(x | D)}\}$
\bigskip

Where $n_{x_i x}$ is the number of times the word $x_i$ is in the document $x$. 

$P(x_i|c, D)$ is defined in the following manner:

\bigskip
$P(x_i|c, D) = \displaystyle \frac{1 + \sum_{d \in D_c} n_{x_i d}}{k + \sum_{j} \sum_{d \in D_c} n_{x_j d}}$
\bigskip

With $D_c$ defined as the collection of documents in the training set of class $c$.
The additional $1$ and $k$ (the cardinality of the vocabulary) are the solutions to the zero-frequency problem.

\subsection{A note about the implementation}

We implemented the Naive Bayes classifier in vanilla C++14 to get a significative speed-up over a Python implementation of the training process. This allowed us to experiments different configurations without losing time in training.


\section{Experimentation}

In order to perform a better classification, we evaluated the performance of training the model using different features classes sets.

The output parameters that we considered fo the evaluation are the following \footnote{Legend: TP = true positives, TN = true negatives, FP = false positives, FN = false negatives}:

\begin{itemize}
\item Precision: $TP / (TP + FP)$
\item Recall: $TP / (TP + FN)$
\item False positives rate: $FP / (FP + TN)$
\item False negatives rate: $FN / (FN + TP)$
\item Accuracy: $(TP + TN) / (TP + FN + TN + FP)$
\end{itemize}

The tested combinations of tags to consider are the following:

\bigskip
\begin{tabular}{ | l | l | }
    \hline
    \textbf{Identifier} & \textbf{Chosen Classes} \\ \hline
    A & \verb|feature permission intent api_call real_permission| \\
    & \verb|activity url call| \\ \hline 
    B & \verb|feature intent api_call real_permission call| \\ \hline 
    C & \verb|api_call feature intent| \\ \hline 
    D & \verb|real_permission api_call| \\ \hline 
    E & \verb|feature intent api_call real_permission| \\ \hline 
    F & \verb|permission url service feature| \\ \hline
\end{tabular}
\bigskip

\subsection{Bernoulli}

In general, the multinomial variant is better than the bernoulli but we tried anyway to confirm this trend also for our case.

The results related to the set A are the following:
\begin{itemize}
\item precision: 0.123560280788
\item recall: 0.978413383702
\item FP rate: 0.312507593983
\item FN rate: 0.0215866162979
\item accuracy: 0.700027904381
\end{itemize}

We stopped the experimentation of the bernoulli here due to the very low accuracy. This model seems to classify a good program as a malware too often and it is worst than a dumb model that returns always false.

\subsection{Multinomial}

Using the multinomial, the results associated with each set are:

\bigskip
\begin{tabular}{ | l | l | l | l | l | l |}
    \hline
    \textbf{Identifier} & \textbf{Precision} & \textbf{Recall} & \textbf{FPR} & \textbf{FNR} & \textbf{Accuracy} \\ \hline
    A & 0.3263 & 0.8537 & 0.07934 & 0.1462 & 0.9177 \\ \hline
    B & 0.5060 & 0.7657 & 0.0336 & 0.2342 & 0.9577 \\ \hline
    C & 0.5911 & 0.6422 & 0.0199 & 0.3577 & 0.9654 \\ \hline
    D & 0.1962 & 0.5779 & 0.1065 & 0.4220 & 0.8798 \\ \hline
    E & 0.5580 & 0.7188 & 0.0256 & 0.2811 & 0.9633 \\ \hline
    F & 0.4198 & 0.8343 & 0.0519 & 0.1656 & 0.9431 \\ \hline
\end{tabular}
\bigskip

As you can see, the accuracy metric is not very effective for the evaluation, in fact, the number of $notmalware$ samples in the dataset is very bigger than the number of $malware$ samples. For instance, even a dumb function that returns always $notmalware$ will reach a high accuracy if using this dataset.

For our purpose, the False Positive Rate and False Negative Rate are the important measures. Obviously to perform a good detection FNR must be reasonably low. Regards FPR we do not want to recognize a good program as malicious.

In addition, FPR is a key measure and must be maintained as low as possible for a very important reason that gets out from statistics: legal issues.
The FPR rate is very important for many malware scanner software, in fact, they prefer to recognize less malware but do not make mistakes with regular software in order to avoid legal actions by software producers.

Precision and Recall are reported for completeness but they can be easily related to FPR and FNR, in fact, recall is simply 1 - FNR and the precision is higher with low FPR values.

\bigskip
\begin{tikzpicture}
    \begin{axis}[
        ymin=0, ymax=0.5,
        xmin=1, xmax=6,
        xtick={1,2,3,4,5,6},
        xticklabels={A,B,C,D,E,F},
        axis x line=bottom,
        axis y line=left,
        title={},
        axis on top=true, clip=false,
        legend pos=outer north east
    ]
    \addplot[mark=*, red] coordinates {
        (1, 0.07934)
        (2, 0.0336)
        (3, 0.0199)
        (4, 0.1065)
        (5, 0.0256)
        (6, 0.0519)
    };
    \addlegendentry{FPR}
    \end{axis}
\end{tikzpicture}
\bigskip


\bigskip
\begin{tikzpicture}
    \begin{axis}[
        ymin=0, ymax=0.5,
        xmin=1, xmax=6,
        xtick={1,2,3,4,5,6},
        xticklabels={A,B,C,D,E,F},
        axis x line=bottom,
        axis y line=left,
        title={},
        axis on top=true, clip=false,
        legend pos=outer north east
    ]
    \addplot[mark=*, blue] coordinates {
        (1, 0.1462)
        (2, 0.2342)
        (3, 0.3577)
        (4, 0.4220)
        (5, 0.2811)
        (6, 0.1656)
    };
    \addlegendentry{FNR}
    \end{axis}
\end{tikzpicture}
\bigskip

Considering this two measures and looking at the proposed graph we can exclude the sets A and D due to the high FPR rate and C and D due to the high FNR rate. F is similar to A but more interesting due to the lower FPR rate than A, but not enough to be considered. The remaining, B and E, have quite similar results but we choose E because, as said before, the FPR rate must be as low as possible.

An interesting property of E is that the feature classes are probably independents so the multinomial model works well with this choice.

\section{Conclusion}

The sets of tags tested are only 5 and so a better combination of tags can be found with further investigations.
Find the best combination over all possible sets is not trivial and it is not worth the trouble due to the fact that significative better results can be achieved only using a more sophisticated model live Not Linear Support Vector Machine.

As a final thought, we want to discuss the problem that we have addressed itself. Its definition is not simple, in fact, the line that divides the $malware$ class from $notmalware$ is not well marked. Consider a program that does not perform harmful actions but contains a particular set of features specifically crafted to elude our classifier. Such program that wants to be detected as a malware, is really a malware or not? Eluding a detector can be considered a malicious behavior? This definition problem is an open problem in computer security.



\vfill
\bibliography{main} 
\bibliographystyle{ieeetr}

\end{document}
